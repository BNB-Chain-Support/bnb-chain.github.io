"use strict";(self.webpackChunkbeta_BNB_Docs=self.webpackChunkbeta_BNB_Docs||[]).push([[2939],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>f});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),h=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},c=function(e){var t=h(e.components);return n.createElement(s.Provider,{value:t},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),d=h(a),g=i,f=d["".concat(s,".").concat(g)]||d[g]||p[g]||r;return a?n.createElement(f,l(l({ref:t},c),{},{components:a})):n.createElement(f,l({ref:t},c))}));function f(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,l=new Array(r);l[0]=g;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[d]="string"==typeof e?e:i,l[1]=o;for(var h=2;h<r;h++)l[h]=a[h];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}g.displayName="MDXCreateElement"},73467:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>h});var n=a(87462),i=(a(67294),a(3905));const r={sidebar_label:"Data Availability Challenge",sidebar_position:2,hide_table_of_contents:!1},l="Data Availability Challenge",o={unversionedId:"greenfield/tech-specs/data-availability-challenge",id:"greenfield/tech-specs/data-availability-challenge",title:"Data Availability Challenge",description:"It is always the first priority of any decentralized storage network to",source:"@site/docs/greenfield/tech-specs/data-availability-challenge.md",sourceDirName:"greenfield/tech-specs",slug:"/greenfield/tech-specs/data-availability-challenge",permalink:"/docs/greenfield/tech-specs/data-availability-challenge",draft:!1,editUrl:"https://github.com/bnb-chain/bnb-chain.github.io/blob/master/docs/greenfield/tech-specs/data-availability-challenge.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_label:"Data Availability Challenge",sidebar_position:2,hide_table_of_contents:!1}},s={},h=[{value:"The Initial Data Integrity and Redundancy Metadata",id:"the-initial-data-integrity-and-redundancy-metadata",level:2},{value:"Data Availability Challenge Process",id:"data-availability-challenge-process",level:2}],c={toc:h},d="wrapper";function p(e){let{components:t,...r}=e;return(0,i.kt)(d,(0,n.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"data-availability-challenge"},"Data Availability Challenge"),(0,i.kt)("p",null,"It is always the first priority of any decentralized storage network to\nguarantee data integrity and availability. Many of the existing\nsolutions rely on intensive computing to generate proofs. However,\nGreenfield chooses the path of social monitoring and challenges."),(0,i.kt)("p",null,"The holistic target for Greenfield is to ensure that the storage provider(SP) stores the data as expected are as below:"),(0,i.kt)("p",null,"   ",(0,i.kt)("strong",{parentName:"p"},"a.")," The primary SP splits the original object that the user uploads into segments correctly."),(0,i.kt)("p",null,"   ",(0,i.kt)("strong",{parentName:"p"},"b.")," The primary SP encodes the segments into redundant Erasing Code pieces correctly, and distributes them to the secondary SP as agreed."),(0,i.kt)("p",null,"   ",(0,i.kt)("strong",{parentName:"p"},"c.")," The SP stores assigned pieces either as the role of primary SP or secondary SP correctly, and the data pieces stored should not be corrupted or falsified."),(0,i.kt)("p",null,"A user needs to ensure that the object stored on Greenfield is really\nhis object without downloading the whole object and comparing the\ncontents. And also, each SP should store the correct piece for each\nobject as required, and this information should be verified on the\nGreenfield blockchain. A special metadata structure is introduced for\nevery object for data challenges as below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-go"},"type ObjectInfo struct {\n    \u2026\n    root         Hash  // primary SP object root, the hash of segments' hashes\n    subRootList []hash //secondary SP object root, the hash of local pieces' hashes\n    \u2026\n}\n")),(0,i.kt)("p",null,"Each storage provider will keep a local manifest for the pieces of each\nobject that are stored on it. For the primary SP, the local manifest\nrecords each segment's hashes. The ",(0,i.kt)("inlineCode",{parentName:"p"},"root")," field of the object's\nmetadata in the above code stores the hash of the whole local manifest\nof the primary SP, e.g., it is the ",(0,i.kt)("inlineCode",{parentName:"p"},"PiecesRootHash(SP0)")," in the below\ndiagram. For the secondary SPs, the local manifest records each piece's\nhashes, and the hash of their local manifest files are recorded in the\nsubRooList field in order, e.g. the 4th element of this list will store\nthe 4th secondary SP's ",(0,i.kt)("inlineCode",{parentName:"p"},"PiecesRootHash(SP4)")," in the below diagram."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"hashes-for-data-integrity",src:a(22023).Z,width:"1046",height:"642"})),(0,i.kt)("div",{align:"center"},(0,i.kt)("i",null,"Hashes for Data Integrity")),(0,i.kt)("p",null,"These root hashes serve as the checksum for the data segments and\nredundancy pieces."),(0,i.kt)("h2",{id:"the-initial-data-integrity-and-redundancy-metadata"},"The Initial Data Integrity and Redundancy Metadata"),(0,i.kt)("p",null,"The user-side client software will perform some work:"),(0,i.kt)("p",null,"   ",(0,i.kt)("strong",{parentName:"p"},"1.")," Split the object file into segments if necessary;"),(0,i.kt)("p",null,"   ",(0,i.kt)("strong",{parentName:"p"},"2.")," Compute the root hash across all the segments;"),(0,i.kt)("p",null,"   ",(0,i.kt)("strong",{parentName:"p"},"3.")," Compute the EC and calculate the hashes for the parity pieces;"),(0,i.kt)("p",null,"   ",(0,i.kt)("strong",{parentName:"p"},"4.")," Send transactions to the Greenfield blockchain to request creating the object with the above information."),(0,i.kt)("p",null,'Besides sending the information to the Greenfield blockchain, the client\nsoftware also sends the same to the primary SP and uploads the payload\ndata onto it. For the primary SP stores the original segments of the\nobject, the SP has to verify the root hash to check the integrity of the\nsegment. The SP also has to compute the EC pieces by itself and verify\nthe hash. All the hashes will be recorded on a manifest file stored\nlocally with the SP, and the root hash of the file will be submitted to\nthe Greenfield blockchain in the "Seal" transaction. Greenfield\nblockchain will verify the hashes in the Seal transaction against the\nobject creation request transaction to ensure data integrity as they are\nthe agreed value across Primary SPs and the users.'),(0,i.kt)("p",null,"These hashes and the corresponding manifest files will be used to verify\nthe data in the data availability challenge as described below."),(0,i.kt)("h2",{id:"data-availability-challenge-process"},"Data Availability Challenge Process"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"data-availability-challenge",src:a(92297).Z,width:"1190",height:"746"})),(0,i.kt)("div",{align:"center"},(0,i.kt)("i",null,"Data Availability Challenge")),(0,i.kt)("p",null,"This data availability challenge is illustrated in figure 19.2 above."),(0,i.kt)("p",null,"The Greenfield validators have the responsibility to verify the data\navailability from the SPs. They form a voting committee to execute this\ntask by the incentive of fees and potential fines (slashes) on SPs."),(0,i.kt)("p",null,"A multi-signing logic, e.g., BLS-based multi-sig, is used to reach\nanother level of off-chain consensus among the Greenfield validators.\nWhen the validator votes for the data challenge, they co-sign an\nattestation and submit on-chain."),(0,i.kt)("p",null,"The overall data availability challenge mechanism works as below:"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"1.")," Anyone can submit a transaction to challenge data availability. The challenge information will be written into the on-chain event triggered after the transaction is processed."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"2.")," The second way to trigger the challenge is more common: at the end of the block process phase of each block, Greenfield will use a random algorithm to generate some data availability challenge events. All challenge information will be persisted on the chain until the challenge has been confirmed or expired."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"3.")," Each validator should run an off-chain data availability check module. This program keeps tracking the on-chain challenge information and then initiates an off-chain data availability check. It checks whether a data piece is available in the specified SP in response to the challenge event, no matter whether the event is triggered by the individual challenger or the Greenfield chain itself. There are three steps to perform the"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Check:")),(0,i.kt)("p",{parentName:"li"}," ",(0,i.kt)("strong",{parentName:"p"},"1.")," Ask the challenged SP for this data piece and the local manifest of the object. If the expected data can't be downloaded, the piece should be regarded as unavailable."),(0,i.kt)("p",{parentName:"li"}," ",(0,i.kt)("strong",{parentName:"p"},"2.")," Compute the hash of the local manifest, and compare it with the related root hash that is recorded in the metadata of the\nobject. If they are different, the piece should be regarded as\nunavailable."),(0,i.kt)("p",{parentName:"li"}," ",(0,i.kt)("strong",{parentName:"p"},"3.")," Compute the checksum hash of the challenged piece, and compare it with the related checksum that is recorded in the local\nmanifest. If they are different, the piece should be regarded\nas unavailable."),(0,i.kt)("p",{parentName:"li"},' Any of the above "',(0,i.kt)("strong",{parentName:"p"},"unavailable"),'" cases will mark the challenge success that the data is unavailable, and the validator will vote "',(0,i.kt)("strong",{parentName:"p"},"unavailable"),'".'))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"4.")," The validator uses its BLS private key to sign a data challenge vote according to the result. The data to vote should be the same for all validators to sign: it should include the block header of the block that contains the challenge, data challenge information, and the challenge result."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"5.")," The data availability challenge votes are propagated through the P2P network."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"6."),' Once a validator collects an agreement from more than 2/3 validators, an "attestment" is concluded. The validator can aggregate the signatures, assemble data challenge attestation, and submit an attestation transaction. In order to solve the concern that validators may just follow the others\' results and not perform the check themselves, a "',(0,i.kt)("strong",{parentName:"p"},"commit-and-reveal"),'" logic will be introduced.'),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"7."),' The data challenge attestation transaction will be executed on-chain. The first validator who submits the attestation can get a submission reward, while the later submission will be rejected. The more votes the submitter aggregates, the more reward it can get. Besides the submission rewards, there are attestment rewards too. Only the validators whose votes wrapped into the attestation will be rewarded, so it may be that some validators voted, but their votes were not assembled by the validator. They won\'t get rewarded for these data availability challenges. Also, for different results, the rewards will be different: the "unavailable" result that slashes the SPs will get validators more rewards.'),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"8.")," After a number of blocks, for example, 100 blocks, the data availability challenge will expire even if the submissions of  attestments haven't arrived. In such a case, the challenge will just expire with no further actions."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"9.")," Once a case of data availability is successfully challenged, i.e. the data is confirmed not available with quality service, there will be a cooling-off period for the SPs to regain, recover, or shift this piece of data."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"10.")," Once the cooling-off period time expires, this data availability can be challenged again if this piece of data is still unavailable, the SP would be slashed again."))}p.isMDXComponent=!0},22023:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/19.1-Hashes-for-Data-Integrity-4606bd526e2d6d237e949a382993be8e.jpg"},92297:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/19.2-Data-Availability-Challenge-1103d47b1d6bca8c31e7e9cfe10532c7.jpg"}}]);